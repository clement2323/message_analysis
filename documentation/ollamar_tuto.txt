

if (!check_ollama())  stop("Ollama is not running. Please start Ollama before proceeding.\n")

# 1) créer un file odelfile.txt dans le quel

FROM C:/Users/RK09OA/Downloads/tinyllama-1.1b-chat-v1.0.Q3_K_S.gguf

# sets the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 1
# sets the context window size to 4096, this controls how many tokens the LLM can use as context to generate the next token
PARAMETER num_ctx 4096

# sets a custom system message to specify the behavior of the chat assistant
SYSTEM résume le message en extirpant les informations importantes

# après ça dans console windows
ollama create tiny-llama -f './modelfile.txt'
ollama run tiny-llama

# le from peut apeler un odel preexistant mais aussi ! un model au format .gguf sur lequel ollama s'appuie

library(ollamar)

Sys.setenv(no_proxy = "localhost,127.0.0.1") # ollama run sur le 11434..
Sys.unsetenv("http_proxy") # remttre les proxy insee pour installation de package seulement
Sys.unsetenv("https_proxy")
test_connection()  # test connection to Ollama server
# bon pour installer les packages mais mauvais pour bosser en local

# le pull marche pas à cause des proxy mais avec le create sur un gguf pers c'est cool
resp <- generate("mistral-small", "tell me a 5-word story") 
resp
# generate a response/text based on a prompt; returns an httr2 response by default
resp <- generate("mistrall-small", "tell me a 5-word story") 

